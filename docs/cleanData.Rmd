---
title: "Data Creation"
author: "Christopher Prener, Ph.D."
date: '(`r format(Sys.time(), "%B %d, %Y")`)'
output: 
  github_document: default
  html_notebook: default 
---

## Introduction
This notebook contains the code for cleaning the 2016-2017 crime file data.

## Dependencies
This notebook requires:

```{r load-packages}
# primary data tools
library(compstatr)     # work with stlmpd crime data

# tidyverse packages
library(dplyr)         # data wrangling
library(readr)         # working with csv data

# spatial packages
library(areal)         # interpolation
library(sf)            # working with spatial data
library(tidycensus)    # census api access
library(tigris)        # tiger/line api access

# other packages
library(janitor)       # frequency tables
library(here)          # file path management
library(knitr)         # output
library(testthat)      # unit testing
```

## Create Data
Data downloaded from the STLMPD website come in `.csv` format but with the wrong file extension. The following bash script copies them to a new subdirectory and fixes the file extension issue:

```{bash}
# change working directory
cd ..
# execute cleaning script
bash source/reformatHTML.sh
```

## Load Data
With our data renamed, we build a year list objects for 2016 and 2017 crimes:

```{r load-data}
data2016 <- cs_load_year(here("data", "intermediate", "2016"))
data2017 <- cs_load_year(here("data", "intermediate", "2017"))
data2018 <- cs_load_year(here("data", "intermediate", "2018"))
```

## Validate Data
The data validation process is currently broken in `compstatr` due to changes in how SLMPD is shipping their data. We'll get these data to a minimum level of validation before proceeding, but they won't fully pass `compstatr`'s tests.

### 2016
Next we make sure there are no problems with the crime files in terms of incongruent columns for 2015:

```{r validate-data16}
cs_validate_year(data2016, year = "2016")
```

We can use the `verbose = TRUE` option on `cs_validate_year()` to identify areas where the validation checks have failed:

```{r validate-data16-v}
cs_validate_year(data2016, year = "2016", verbose = TRUE)
```

Since there are only issues with the class validation, we're going to consider these validated for now.

### 2017
We'll repeat the same validation for 2017:

```{r validate-data17}
cs_validate_year(data2017, year = "2017")
```

We can use the `verbose = TRUE` option on `cs_validate_year()` to identify areas where the validation checks have failed:

```{r validate-data17-v}
cs_validate_year(data2017, year = "2017", verbose = TRUE)
```

The data for May 2017 do not pass the validation checks. We can extract this month and confirm that there are too many columns in the May 2017 release. Once we have that confirmed, we can standardize that month and re-run our validation.

```{r fix-may-cols}
# extract data
may2017 <- cs_extract_month(data2017, month = "May")
# unit test column number
expect_equal(ncol(may2017), 26)
# remove object
rm(may2017)
# standardize months
data2017 <- cs_standardize(data2017, month = "May", config = 26)
# validate data
cs_validate_year(data2017, year = "2017")
```

We still get a `FALSE` value for `cs_validate_year()`:

```{r validate-data17-v2}
cs_validate_year(data2017, year = "2017", verbose = TRUE)
```

We've now limited the validation issues to the classes.

### 2018
Finally, we'll validate the 2018 data, which contain some reported crimes for 2016 and 2017 as well:

```{r validate-data18}
cs_validate_year(data2018, year = "2018")
```

Now with the `verbose = TRUE` option:

```{r validate-data18-v}
cs_validate_year(data2018, year = "2018", verbose = TRUE)
```

We only have issues with the class validation.

## Collapse Data
With the data validated, we collapse each year into a single, flat object:

```{r collapse-data}
data2016_flat <- cs_collapse(data2016)
data2017_flat <- cs_collapse(data2017)
data2018_flat <- cs_collapse(data2018)
```

What we need for this project is a single object with only the crimes for 2016. Since crimes were *reported* in subsequent years for 2015 (as well as 2016 and 2017), we need to merge all the tables and then retain only the relevant year's data. The `cs_combine()` function will do this:

```{r combine-data}
crimes2016 <- cs_combine(type = "year", date = 2016, data2016_flat, data2017_flat, data2018_flat)
crimes2017 <- cs_combine(type = "year", date = 2017, data2017_flat, data2018_flat)
```

### Clean-up Environment
With our data created, we can remove some of the intermediary objects we've created:

```{r rm-initial-objects}
rm(data2016, data2016_flat, data2017, data2017_flat, data2018, data2018_flat)
```

## Remove Unfounded Crimes and Subset Based on Type of Crime:
The following code chunk removes unfounded crimes (those where `Count == -1`) and then creates a data frame for all part one crimes for each year. We also print the number of crimes missing spatial data. In general, these tend to be rapes. We're focusing on Part 1 Crimes, which are defined by the FBI as all violent crimes (aggravated assault, rape, murder, and robbery) as well as property crimes (arson, burglary, larceny-theft, and motor vehicle theft).

### 2016
Our initial task to is subset the data. We also add a column categorizing each crime, and select only the columns we need for this mapping project:

```{r subset-data-16}
crimes2016 %>% 
  cs_filter_count(var = Count) %>%
  cs_filter_crime(var = Crime, crime = "Part 1") %>%
  cs_crime_cat(var = Crime, newVar = crimeCat, output = "string") %>%
  cs_missing_xy(varx = XCoord, vary = YCoord, newVar = xyStatus) %>%
  select(Complaint, DateOccur, Crime, crimeCat, Description, 
         ILEADSAddress, ILEADSStreet, XCoord, YCoord, xyStatus) -> p1_2016
```

Next, we'll print the number of crimes that are missing spatial data:

```{r table-missingXY-2016}
p1_2016 %>%
  tabyl(xyStatus) %>%
  adorn_pct_formatting(digits = 3) %>%
  kable()
```

Finally, we'll summarize the crimes so that we get a sense of how much crime falls into specific categories:

```{r table-crimeCat-2016}
p1_2016 %>%
  tabyl(crimeCat) %>%
  adorn_pct_formatting(digits = 3) %>%
  kable()
```

### 2017
For 2017, we'll again subset and clean the data:

```{r subset-data-17}
crimes2017 %>% 
  cs_filter_count(var = Count) %>%
  cs_filter_crime(var = Crime, crime = "Part 1") %>%
  cs_crime_cat(var = Crime, newVar = crimeCat, output = "string") %>%
  cs_missing_xy(varx = XCoord, vary = YCoord, newVar = xyStatus) %>%
  select(Complaint, DateOccur, Crime, crimeCat, District, Description, 
         ILEADSAddress, ILEADSStreet, XCoord, YCoord, xyStatus) -> p1_2017
```

Next, we'll print the number of crimes that are missing spatial data:

```{r table-missingXY-2017}
p1_2017 %>%
  tabyl(xyStatus) %>%
  adorn_pct_formatting(digits = 3) %>%
  kable()
```

Finally, we'll summarize the crimes so that we get a sense of how much crime falls into specific categories:

```{r table-crimeCat-2017}
p1_2017 %>%
  tabyl(crimeCat) %>%
  adorn_pct_formatting(digits = 3) %>%
  kable()
```

## Clean Environment
We no longer need the full crime objects, so we'll get rid of those now:

```{r rm-crime-objects}
rm(crimes2016, crimes2017)
```

## Combine Objects
Since the Belmar proposal mapped the data for two years, we need to bind our two data objects together.

```{r create-single-object}
p1_crimes <- bind_rows(p1_2016, p1_2017)
```

## Write Data
Finally, we'll creat a spreadsheet of our data for later reference:

```{r write-data}
write.csv(p1_crimes, here("data", "clean", "p1_crimes.csv"))
```
